{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0bf911b-f439-4466-829a-4a43040d0593",
   "metadata": {},
   "source": [
    "### Text Classification \n",
    "Goal: to gain experience with natural language processing libraries like nltk and spacy, and simple prediction models in sklearn\r\n",
    "\n",
    "Project Scope: \r\n",
    "This project aims to predict an individual's nationality based on information from her/his Wikipedia bio. One challenge is that the Wikipedia training set nationality fields can be noisy, so one goal is to use nltk or spacy to clean the target field (rather than using manual techniques).\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61ff03dd-6542-4fce-bbf0-e079c12c081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "# from nltk.stem import SnowballStemmer  #using lemmatization instead\n",
    "# sbs = SnowballStemmer(\"english\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c37f9c7-507a-4ef9-8b63-17e63b33372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 655"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c99c050-3d3b-47c4-b142-e5cd310d7a58",
   "metadata": {},
   "source": [
    "### Initial Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ba1064c9-3226-49f9-bf37-c6124764d305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 319358 records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bio</th>\n",
       "      <th>nationality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>266787</th>\n",
       "      <td>Markus Irle (born 5 February 1976) is an Austr...</td>\n",
       "      <td>austrian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63181</th>\n",
       "      <td>Dino Domenico Natali (better known as Dino Nat...</td>\n",
       "      <td>united states, american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313518</th>\n",
       "      <td>Life\\nIn 1924 Bausch was co-founder of the Pro...</td>\n",
       "      <td>german</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      bio  \\\n",
       "266787  Markus Irle (born 5 February 1976) is an Austr...   \n",
       "63181   Dino Domenico Natali (better known as Dino Nat...   \n",
       "313518  Life\\nIn 1924 Bausch was co-founder of the Pro...   \n",
       "\n",
       "                    nationality  \n",
       "266787                 austrian  \n",
       "63181   united states, american  \n",
       "313518                   german  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nationality_df = pd.read_csv('bio_nationality.tsv.gz', sep='\\t', compression='gzip', index_col=0)\n",
    "nationality_df.dropna(inplace = True)  #### skip empty entries (drop 11)\n",
    "print(f\"The dataset has {nationality_df.shape[0]} records\")\n",
    "nationality_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7f4bcad3-9c80-41d6-bea7-056697ea4562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nationality\n",
       "american                   13.51\n",
       "british                     5.21\n",
       "indian                      3.37\n",
       "united states, american     3.07\n",
       "australian                  2.59\n",
       "french                      2.31\n",
       "german                      2.08\n",
       "italian                     1.69\n",
       "japanese                    1.64\n",
       "canadian                    1.59\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(nationality_df.nationality.value_counts().head(10)/nationality_df.shape[0]*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477402c5-c43d-43a0-bf48-0c321ef71b6d",
   "metadata": {},
   "source": [
    "Exploratory observations:\n",
    "* there is some inconsistency in the way nationalities are represented, e.g., \"american\" vs \"united states, american\".\n",
    "* some nationality data is garbled or lists more than one nationality, e.g., \"norway, norwegian and germany, german\" or \"rus) (pol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "52f1958b-1e3f-4327-9a1f-d44ddb95fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nationality_df.nationality.value_counts().iloc[61:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b5aa0a-7a9c-4188-a8c8-552901615d47",
   "metadata": {},
   "source": [
    "Two techniques are explored to clean the nationality field:\n",
    "1. manual transformations based of some of the main type of observed issues\n",
    "2. using spacy named entity recognition directly\n",
    "3. using a combination of approaches 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cd3a5d-e78c-4515-bbd7-4c33b8f7dd07",
   "metadata": {},
   "source": [
    "##### Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c2866734-ae1d-4530-9231-6a9d01e91951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this approach was unable to identify nationality for 1.02% of the records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bio</th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_basic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85404</th>\n",
       "      <td>Jennifer Rardin (April 28, 1965 – September 20...</td>\n",
       "      <td>american</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191167</th>\n",
       "      <td>Thomas Joris Paul Alizier (born June 20, 1990)...</td>\n",
       "      <td>french people, french</td>\n",
       "      <td>french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264169</th>\n",
       "      <td>Marie-Thérèse Abena Ondoa (nee '''Obama''') is...</td>\n",
       "      <td>cameroonian</td>\n",
       "      <td>cameroonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98679</th>\n",
       "      <td>Political career\\nMnqasela was initially an Af...</td>\n",
       "      <td>south african</td>\n",
       "      <td>south african</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242017</th>\n",
       "      <td>Biography\\nAlexandra Vela was born to Ecuadori...</td>\n",
       "      <td>flag: ecuador</td>\n",
       "      <td>flag: ecuador</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      bio  \\\n",
       "85404   Jennifer Rardin (April 28, 1965 – September 20...   \n",
       "191167  Thomas Joris Paul Alizier (born June 20, 1990)...   \n",
       "264169  Marie-Thérèse Abena Ondoa (nee '''Obama''') is...   \n",
       "98679   Political career\\nMnqasela was initially an Af...   \n",
       "242017  Biography\\nAlexandra Vela was born to Ecuadori...   \n",
       "\n",
       "                  nationality nationality_basic  \n",
       "85404                american          american  \n",
       "191167  french people, french            french  \n",
       "264169            cameroonian       cameroonian  \n",
       "98679           south african     south african  \n",
       "242017          flag: ecuador     flag: ecuador  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nationality_cleanup_basic(x, basic = True):\n",
    "    \"\"\"This function tried to manually clean the nationality field based on the most common types of errors seen. This is a brute force approach\n",
    "    and any nuances in the nationality field are lost\"\"\"\n",
    "    \n",
    "    nationalities = x.split(\",\")\n",
    "    if len(nationalities) ==  1:\n",
    "        _ = nationalities[0].strip()\n",
    "    elif len(nationalities) ==  2:\n",
    "        _ = nationalities[1].strip()\n",
    "    else: \n",
    "        _ = None\n",
    "        \n",
    "    if _ in [\"us\", \"united states\", \"usa\", \"U.S.A.\"]:\n",
    "        _ = \"american\"\n",
    "    elif _ in [\"united kingdom\", \"uk\", \"english\", \"eng\"]:\n",
    "        _ = \"british\"\n",
    "\n",
    "    if basic == False and _ == None:\n",
    "        return x\n",
    "    else:    return _\n",
    "\n",
    "start = time.time()\n",
    "nationality_df[\"nationality_basic\"] = nationality_df.nationality.apply(lambda x: nationality_cleanup_basic(x, basic = True))\n",
    "print(f\"this approach was unable to identify nationality for {round(nationality_df.nationality_basic.isna().sum()/nationality_df.shape[0]*100,2)}% of the records\")\n",
    "nationality_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b96a7639-9cde-4576-b374-896bb3a7be96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nationality_basic\n",
       "american      21.11\n",
       "british        9.23\n",
       "indian         4.01\n",
       "french         3.04\n",
       "german         2.75\n",
       "canadian       2.71\n",
       "australian     2.70\n",
       "italian        2.30\n",
       "japanese       1.95\n",
       "spanish        1.31\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(nationality_df.nationality_basic.value_counts().head(10)/ nationality_df.shape[0]*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263caaa-63df-44a8-9db1-6b002ba6f55b",
   "metadata": {},
   "source": [
    "Simple clean up improves the nationality groupings quite a bit and is only unable to clean up about 1% of the records. \n",
    "\n",
    "But this approach requires that we know what are some of the common types of errors in the nationality field ahead of time, which is not always possible. Therefore, the second approach, that extracts nationalities directly using spacy is utilized\n",
    "\n",
    "##### Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "945be485-19a8-4200-9fe8-96b04e6c1033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy based cleanup takes ~24.82m to run\n",
      "this approach was unable to identify nationality for 3.17% records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bio</th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_basic</th>\n",
       "      <th>nationality_spacy_direct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alain Connes (born 1 April 1947) is a French m...</td>\n",
       "      <td>french</td>\n",
       "      <td>french</td>\n",
       "      <td>french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Life\\n=== Early life ===\\nSchopenhauer's birth...</td>\n",
       "      <td>german</td>\n",
       "      <td>german</td>\n",
       "      <td>german</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life and career\\nAlfred Nobel at a young age i...</td>\n",
       "      <td>swedish</td>\n",
       "      <td>swedish</td>\n",
       "      <td>swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Early life\\nAlfred Vogt (both \"Elton\" and \"van...</td>\n",
       "      <td>canadian</td>\n",
       "      <td>canadian</td>\n",
       "      <td>canadian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alfons Maria Jakob (2 July 1884 in Aschaffenbu...</td>\n",
       "      <td>german</td>\n",
       "      <td>german</td>\n",
       "      <td>german</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 bio nationality  \\\n",
       "0  Alain Connes (born 1 April 1947) is a French m...      french   \n",
       "1  Life\\n=== Early life ===\\nSchopenhauer's birth...      german   \n",
       "2  Life and career\\nAlfred Nobel at a young age i...     swedish   \n",
       "3  Early life\\nAlfred Vogt (both \"Elton\" and \"van...    canadian   \n",
       "4  Alfons Maria Jakob (2 July 1884 in Aschaffenbu...      german   \n",
       "\n",
       "  nationality_basic nationality_spacy_direct  \n",
       "0            french                   french  \n",
       "1            german                   german  \n",
       "2           swedish                  swedish  \n",
       "3          canadian                 canadian  \n",
       "4            german                   german  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "def nationality_cleanup_spacy(x):\n",
    "    \"additional clean up to extract nationalities. only the first nationality extracted is returned (in cases where many are extracted)\"\n",
    "    \n",
    "    doc = nlp(x)\n",
    "    if len(doc) == 1: ##if we only have one nationality specified, we used it directly\n",
    "        return list({word.lemma_ for word in doc})[0]\n",
    "        # return {sbs.stem(x)}\n",
    "        \n",
    "    else: ##if we have additional information, we try to extract the nationality using spacy NORP (\"Nationalities or religious or political groups\")\n",
    "        # https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "        \n",
    "        NORP_GPE_extract = [(X.text, X.label_) for X in doc.ents]\n",
    "        nationalities = list({lemmatizer.lemmatize(X[0]) for X in NORP_GPE_extract if X[1] == \"NORP\"})\n",
    "        countries = list({lemmatizer.lemmatize(X[0]) for X in NORP_GPE_extract if X[1] == \"GPE\"})\n",
    "        # nationalities = {sbs.stem(X[0] for X in NORP_GPE_extract if X[1] == \"NORP\"}\n",
    "        # countries = {sbs.stem(X[0]) for X in NORP_GPE_extract if X[1] == \"GPE\"}\n",
    "        \n",
    "        if len(nationalities) >= 1:\n",
    "            return nationalities[0]\n",
    "        elif len(countries) >= 1: ##if no nationality information could be effectively extracted, we try to extract country names\n",
    "            return countries[0]\n",
    "        else: return None\n",
    "\n",
    "nationality_df[\"nationality_spacy_direct\"] = nationality_df.nationality.apply(lambda x: nationality_cleanup_spacy(x))\n",
    "print(f\"spacy based cleanup takes ~{round((time.time() - start)/60,2)}m to run\")\n",
    "print(f\"this approach was unable to identify nationality for {round(nationality_df.nationality_spacy_direct.isna().sum()/nationality_df.shape[0]*100,2)}% records\")\n",
    "nationality_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cfc1703b-2c6f-41c9-b3fb-2cb5c9f0aeac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nationality_spacy_direct\n",
       "american         18.89\n",
       "british           7.24\n",
       "indian            4.15\n",
       "french            3.28\n",
       "australian        3.14\n",
       "german            2.98\n",
       "canadian          2.96\n",
       "italian           2.47\n",
       "japanese          2.01\n",
       "united states     1.67\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(nationality_df.nationality_spacy_direct.value_counts().head(10)/ nationality_df.shape[0]*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63efad07-ffae-4b7c-99cb-e360cedab519",
   "metadata": {},
   "source": [
    "Spacy fails to identify nationalities for about 3% of the records (compared to 1% via direct transformations) and the results obtain from spacy based nationality extraction also contain some noise (e.g., american and united states are separate entities). But in general, the results are comparable especially when we consider that spacy did not require any prior information on the types of noise observed in the nationality column, and was able to extract the relevant nationalities independently. \n",
    "\n",
    "##### Approach 3\n",
    "Both techniques are combined in a final third approach where we manually correct any obvious/known issues (such as those discussed in in the introduction) and then apply spacy. This approach is able to identify the nationalities for about 98% of the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a3749db7-6c4e-4349-8925-99f582d0f779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy based cleanup takes ~55.51m to run\n",
      "this approach was unable to identify nationality for 1.99% records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bio</th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_basic</th>\n",
       "      <th>nationality_spacy_direct</th>\n",
       "      <th>nationality_basic2</th>\n",
       "      <th>nationality_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alain Connes (born 1 April 1947) is a French m...</td>\n",
       "      <td>french</td>\n",
       "      <td>french</td>\n",
       "      <td>french</td>\n",
       "      <td>french</td>\n",
       "      <td>french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Life\\n=== Early life ===\\nSchopenhauer's birth...</td>\n",
       "      <td>german</td>\n",
       "      <td>german</td>\n",
       "      <td>german</td>\n",
       "      <td>german</td>\n",
       "      <td>german</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life and career\\nAlfred Nobel at a young age i...</td>\n",
       "      <td>swedish</td>\n",
       "      <td>swedish</td>\n",
       "      <td>swedish</td>\n",
       "      <td>swedish</td>\n",
       "      <td>swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Early life\\nAlfred Vogt (both \"Elton\" and \"van...</td>\n",
       "      <td>canadian</td>\n",
       "      <td>canadian</td>\n",
       "      <td>canadian</td>\n",
       "      <td>canadian</td>\n",
       "      <td>canadian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alfons Maria Jakob (2 July 1884 in Aschaffenbu...</td>\n",
       "      <td>german</td>\n",
       "      <td>german</td>\n",
       "      <td>german</td>\n",
       "      <td>german</td>\n",
       "      <td>german</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 bio nationality  \\\n",
       "0  Alain Connes (born 1 April 1947) is a French m...      french   \n",
       "1  Life\\n=== Early life ===\\nSchopenhauer's birth...      german   \n",
       "2  Life and career\\nAlfred Nobel at a young age i...     swedish   \n",
       "3  Early life\\nAlfred Vogt (both \"Elton\" and \"van...    canadian   \n",
       "4  Alfons Maria Jakob (2 July 1884 in Aschaffenbu...      german   \n",
       "\n",
       "  nationality_basic nationality_spacy_direct nationality_basic2  \\\n",
       "0            french                   french             french   \n",
       "1            german                   german             german   \n",
       "2           swedish                  swedish            swedish   \n",
       "3          canadian                 canadian           canadian   \n",
       "4            german                   german             german   \n",
       "\n",
       "  nationality_spacy  \n",
       "0            french  \n",
       "1            german  \n",
       "2           swedish  \n",
       "3          canadian  \n",
       "4            german  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "# nationality_df[\"nationality_basic2\"] = nationality_df.nationality.apply(lambda x: nationality_cleanup_basic(x, basic = False))\n",
    "# nationality_df.nationality_basic2.isna().sum()\n",
    "# nationality_df[\"nationality_spacy\"] = nationality_df.nationality_basic2.apply(lambda x: nationality_cleanup_spacy(x))\n",
    "print(f\"spacy based cleanup takes ~{round((time.time() - start)/60,2)}m to run\")\n",
    "print(f\"this approach was unable to identify nationality for {round(nationality_df.nationality_spacy.isna().sum()/nationality_df.shape[0]*100,2)}% records\")\n",
    "# nationality_df.to_csv(\"nationality_df.csv\")\n",
    "nationality_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1d253d74-de86-4099-b2a5-4aa26e75fbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nationality_spacy\n",
       "american      21.62\n",
       "british        9.53\n",
       "indian         4.14\n",
       "french         3.27\n",
       "australian     3.11\n",
       "german         2.97\n",
       "canadian       2.93\n",
       "italian        2.46\n",
       "japanese       2.00\n",
       "spanish        1.39\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(nationality_df.nationality_spacy.value_counts().head(10)/ nationality_df.shape[0]*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "38e7ab76-068f-4169-a10b-412d094a4b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nationality_spacy\n",
       "american      21.62\n",
       "british        9.53\n",
       "indian         4.14\n",
       "french         3.27\n",
       "australian     3.11\n",
       "german         2.97\n",
       "canadian       2.93\n",
       "italian        2.46\n",
       "japanese       2.00\n",
       "spanish        1.39\n",
       "irish          1.30\n",
       "russian        1.30\n",
       "mexican        1.25\n",
       "polish         1.20\n",
       "dutch          1.15\n",
       "norwegian      1.12\n",
       "pakistani      1.11\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nationalities_freq = round(nationality_df.nationality_spacy.value_counts()/nationality_df.shape[0]*100, 2)\n",
    "nationalities_freq_keep = nationalities_freq[nationalities_freq>1] ##narrowing it down to nationalities that have atleast 1% presentation in the dataset\n",
    "nationalities_freq_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7fc58cfd-d777-4ffb-8c7d-7ecd63a2e477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('american (naturalized 1959', 'ORG')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doc= nlp(\"He is an americans from America\")#. The cats are blue\")\n",
    "# # [(X.text, X.label_) for X in doc.ents]\n",
    "# [(lemmatizer.lemmatize(X.text), X.label_) for X in doc.ents]\n",
    "# # {X[0].lemma_ for X in NORP_GPE_extract if X[1] == \"NORP\"}\n",
    "\n",
    "\n",
    "\n",
    "# sbs.stem(\"american\")\n",
    "# x = \"american (naturalized 1959)\"\n",
    "# x = x.replace(\"hungarians, hungarian\")\n",
    "# print(x)\n",
    "# token_list = [token for token in nltk.word_tokenize(x) if token not in punctuation and token not in [\n",
    "#         \"flag\", \"people\", \"name\", \"republic\", \"of\", \"democratic\", \"nationality\", \"law\", \"flagicon\"]]\n",
    "\n",
    "# # token_list = {sbs.stem(i) for i in token_list}\n",
    "# token_list = \" \".join(token_list)\n",
    "# print(token_list)\n",
    "\n",
    "doc = nlp(x) #\"american, american\") #token_list) #(\"myanmar, burmese\")\n",
    "[(X.text, X.label_) for X in doc.ents]\n",
    "\n",
    "# token_list = [token for token in nltk.word_tokenize(\"myanmar, burmese\") if token not in punctuation and token not in [\n",
    "#         \"flag\", \"people\", \"name\", \"republic\", \"of\", \"democratic\", \"nationality\", \"law\", \"flagicon\"]]\n",
    "\n",
    "# # token_list = {sbs.stem(i) for i in token_list}\n",
    "# token_list = \" \".join(token_list)\n",
    "# token_list\n",
    "\n",
    "# # doc = nlp(token_list) #(\"myanmar, burmese\")\n",
    "# # [(X.text, X.label_) for X in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d9e3eb0e-bc22-448e-baa3-ee1bf9f774b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bio</th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_clean</th>\n",
       "      <th>nationality_clean2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alain Connes (born 1 April 1947) is a French m...</td>\n",
       "      <td>french</td>\n",
       "      <td>french</td>\n",
       "      <td>{french}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Life\\n=== Early life ===\\nSchopenhauer's birth...</td>\n",
       "      <td>german</td>\n",
       "      <td>german</td>\n",
       "      <td>{german}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life and career\\nAlfred Nobel at a young age i...</td>\n",
       "      <td>swedish</td>\n",
       "      <td>swedish</td>\n",
       "      <td>{swedish}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Early life\\nAlfred Vogt (both \"Elton\" and \"van...</td>\n",
       "      <td>canadian</td>\n",
       "      <td>canadian</td>\n",
       "      <td>{canadian}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alfons Maria Jakob (2 July 1884 in Aschaffenbu...</td>\n",
       "      <td>german</td>\n",
       "      <td>german</td>\n",
       "      <td>{german}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 bio nationality  \\\n",
       "0  Alain Connes (born 1 April 1947) is a French m...      french   \n",
       "1  Life\\n=== Early life ===\\nSchopenhauer's birth...      german   \n",
       "2  Life and career\\nAlfred Nobel at a young age i...     swedish   \n",
       "3  Early life\\nAlfred Vogt (both \"Elton\" and \"van...    canadian   \n",
       "4  Alfons Maria Jakob (2 July 1884 in Aschaffenbu...      german   \n",
       "\n",
       "  nationality_clean nationality_clean2  \n",
       "0            french           {french}  \n",
       "1            german           {german}  \n",
       "2           swedish          {swedish}  \n",
       "3          canadian         {canadian}  \n",
       "4            german           {german}  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nationality_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323f342-cc8c-4528-8ab4-8c296c49dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nationality_df[nationality_df.nationality_basic.isna()].shape[0]/nationality_df.shape[0]\n",
    "# nationality_df[nationality_df.nationality_clean3.isna()].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945faf5-bafd-4e35-ab10-487f57154f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nationality_df = pd.read_csv(\"nationality_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004c5fb-81e5-4b45-9666-822cc0fa5490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nationality_cleanup_basic(x):\n",
    "#     \"some basic clean up for the nationality field\"\n",
    "    \n",
    "#     ##neither nltk nor spacy lemmatizers or stemmers worked so it appears nationality needs to be cleaned up manually\n",
    "#     x = x.replace(\"united states\", \"american\")\n",
    "#     x = x.replace(\"u.s.\", \"american\")\n",
    "#     x = x.replace(\"usa\", \"american\")\n",
    "#     x = x.replace(\"united states of america\", \"american\")\n",
    "#     x = x.replace(\"united kingdom\", \"british\")\n",
    "#     x = x.replace(\"england\", \"british\")\n",
    "#     x = x.replace(\"english\", \"british\")\n",
    "#     # x = x.replace(\"india\", \"indian\")\n",
    "#     x = x.replace(\"france\", \"french\")\n",
    "#     x = x.replace(\"belgium\", \"belgian\")\n",
    "#     x = x.replace(\"jpn\", \"japanese\")\n",
    "#     x = x.replace(\"netherlands\", \"dutch\")\n",
    "#     x = x.replace(\"switzerland\", \"swiss\")\n",
    "#     x = x.replace(\"puerto ric\", \"puertoric\")\n",
    "#     # x = x.replace(\"south africa\", \"southafrica\")\n",
    "#     # x = x.replace(\"new zealand\", \"newzealand\")\n",
    "#     # x = x.replace(\"north korea\", \"northkorea\")\n",
    "#     # x = x.replace(\"south korea\", \"southkorea\")\n",
    "    \n",
    "#     token_list = [token for token in nltk.word_tokenize(x) if token not in punctuation and token not in [\n",
    "#         \"flag\", \"people\", \"name\", \"republic\", \"of\", \"democratic\", \"nationality\", \"law\", \"flagicon\"]]\n",
    "\n",
    "#     return \" \".join(token_list)\n",
    "\n",
    "# nationality_df[\"nationality_basic\"] = nationality_df.nationality.apply(lambda x: nationality_cleanup_basic(x))\n",
    "# nationality_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
